## Comentario: Multi-Armed Recommender System Bandit Ensembles

El paper plantea un ensamble de sistemas recomendadores basado en un _bandit_ múltiples brazos, en donde cada uno representa un sistema recomendador. Se elige qué brazo utilizar para una recomendación según un algoritmo, en este caso utilizaron epsilon-greedy y Thomson, el cual se basa en las recompensas (si le gustó la recomendación al usuario o no). También plantea que la manera de evaluar el ensamble será cíclica: se utiliza el feedback de los usuarios a las recomendaciones para mejorar el sistema y realizar mejores recomendaciones. Esto tiene la ventaja de que permite evaluar el sistema recomendador en un escenario más realista, pudiendo estudiar su desempeño no solo en la tarea de recomendar según un dataset estático previo.

No queda claro el proceso de entrenamiento. Utilizan un _split_ de 5% para set de training y 95% para set de testing, diciendo que en cada época se produce una recomendación por cada usuario, la cual tiene una recompensa asociada que permite ajustar el algoritmo de selección de recomendación. Sin embargo, no queda claro cómo los recomendadores son capaces de 'mejorar' en el tiempo: ¿En cada época se produce un nuevo split 5/95 para entrenar y evaluar? Si fuera así, se pueden producir problemas y los recomendadores podrían estar mejorando solo porque ya se entrenaron con datos que ahora están en el set de testing. También se plantea que cada recomendador tiene su propio set de training y testing, pero tampoco se explica cómo o por qué se logra esa diferencia.

Relacionado a lo anterior, un split de 5/95 puede generar un dataset excesivamente _sparse_, lo cual influencia el desempeño de recomendadores, pudiendo elegir recomendadores que no son los óptimos en condiciones más reales. Como dicen en el paper, esta puede ser la razón de que _most popular_ tenga un mejor rendimiento que u-knn y factorización matricial, lo cual es sorprendente.

Me pareció interesante que solo utilizaran como feedback los _ratings_ explícitos de los usuarios, y no cambiaban el algoritmo si es que este recomendaba un item que el usuario no había calificado. Esta manera se apega más a la realidad y me parece más adecuada que utilizar los datos desconocidos como feedback negativo, como varios otros papers, porque puede introducir ruido.

Se podría haber ahondado más en el hecho de que el ensamble prefirió factorización matricial sobre u-knn, siendo que u-knn tenía un mejor desempeño individual que factorización matricial. Cuando el objetivo del _bandit_ es elegir el brazo con mejor desempeño, resulta extraño que eligiera factorización matricial.

Finalmente, el análisis de los ensambles dinámicos produce una de las conclusiones más interesantes del paper. Se puede apreciar que los ensambles dinámicos tienen un pobre desempeño, pero, como dicen en el paper, estos ensambles se basan en el método de evaluación más típico utilizado para evaluar sistemas recomendadores hasta ahora. Se tiene un set de training y un set de test, y se elige el método con mejor desempeño en el set de testing. Si un ensamble basado en esta idea tuvo malos resultados, cabe preguntar si los sistemas recomendadores evaluados hasta ahora realmente eran los mejores.

En conclusión, el paper presenta un novedoso ensamble con una evaluación cíclica de recomendaciones. Se podría haber explicado de mejor forma los métodos de entrenamiento, las consecuencias de algunas decisiones de diseño como el _split_ 5/95 y el por qué el _bandit_ preferiría un recomendador que individualmente tiene peor desempeño. Sin embargo, el paper presenta interesantes ideas, como solo utilizar datos conocidos o poder cuestionar el método de evaluación de sistemas recomendadores hasta ahora por el mal desempeño de ensambles dinámicos.