# Commentary: Evaluating Recommendation Systems

In the first part of the chapter the authors talk about different experiment types to measure the performance of a recommender system: offline experiments, based on a testing set extracted from the dataset, user studies, where small groups interact with the system and the investigator can question them about their experience, and online experiments, where the system is launched into production and one can measure the users interactions with it. In the second part they describe different ways to measure the performance of the recommender system: accuracy, coverage, confidence, trust, novelty, serendipity, diversity, utility, risk, robustness, privacy, adaptivity and scalability. They also present how to measure these qualities and when they could be more important than others.

An interesting idea presented in the experiments types section is to divide de training and test data chronologically. In real world scenarios, the recommender system has to predict future user behavior (i.e. if they would like a certain item or which item would be best to recommend) with past user behavior (what they liked or interacted with before), so it makes sense to divide the dataset with the same purpose in mind, which is something I haven't seen any papers do. 

The authors say that a common approach in research papers is to leave a fixed amount of _n_ items per user for the test set, but that "we must ask ourselves whether we are truly interested in presenting recommendations only for users [...] expected to rate exactly _n_ items more". This doesn't make any sense. How is leaving a certain amount for the test set optimizing only for people who are going to rate that amount of movies? Furthermore, that would mean we're optimizing for a condition in the future, for people who are going to do something but that we don't even know if they will. It is a small critique in a very informative and well written text, but this phrase stood out to me as really wrong.

In the subject of recommendation system properties, the paper starts from the base that we know what we want from a recommender system: we want it to be novel, or we want it to be more accurate. Getting to this point, though, seems like a very complex task. Even though we can evaluate the performance of the system with all these properties, it doesn't seem enough and we need one particular property that will reign over the others. For example, if we want to know how novel my recommendations should be, we need to run an experiment where we only change the novelty of the recommendation. To evaluate the results, though, we need another way of measuring performance of the recommender system, maybe movies watched or time spent on the page. So, for most of these properties, they aren't enough on their own and they're a way of tuning a system to get what we actually want. In that sense, it is interesting that most of the research only focuses on accuracy, which is not the real goal of most real world recommender systems and might just easily be another parameter to tune like the rest. 

The approach presented in this paper seems to be about finding _the_ best recommender system of a given circumstance. Find just how much novelty users want or how much privacy are they willing to give up for better recommendations. It ignores the individual preferences, maybe some people are more willing to experiment with new items than others or are less worried about their privacy. Related to this, I find [Steam's recommender](https://store.steampowered.com/recommender/) system approach to novelty interesting. As a user, one can choose if we want more 'popular' or more 'niche' recommendations, leaving the problem of tuning this parameter to the user and what they want in a particular time. Not only does this solve the problem of tuning the parameter but it also gives more options to the user, making it easier to give better recommendations for a particular person. 

In summary, this paper helps with the task of evaluating a recommender system. It provides a simple idea to make a training / test split more real by doing so chronologically. It also describes a series of properties of the systems and how to measure them, but it doesn't provide much help in deciding which properties are the most important. The paper brought to my attention that most research focuses on one specific property (accuracy), usually ignoring the rest, even when they could be more important. Finally, I presented an alternative approach to tuning these properties: leaving the option to the user, which I think is interesting and worth exploring. 
