# Commentary: Performance of Recommender Algorithms on Top-N Recommendation Tasks

This paper studies the performance of SVD and iKnn, algorithms optimized for minimizing RMSE, in top-N ranking tasks. They proposed new versions of SVD and item based collaborative filtering, without the restrictions imposed by calculating a rating. For the most popular items, the traditional methods performed worse than the new items and even worse than Top popular methods. Excluding the most popular items, the traditional algorithms did perform better than top popular ones, but still worse than the new proposed methods.

In its time, this paper was probably revolutionary. It contradicted a common hypothesis: that minimizing RMSE could help in the task of getting the top-N recommendations. Most algorithms were optimized with RMSE in mind, so it meant that the best algorithms were not as good for one of the most common (if not the most common) task recommender systems have to do. 

Thanks to the shift in approach, away from a rating prediction, the paper could do some very simple modifications to common algorithms to achieve much better performance in top-N tasks. Not limiting the predicted rating with iKnn to [1, 5] meant they could remove the denominator and the rating would be higher when there are more similar neighbors. Something similar could be applied to the task of predicting ratings, using this uncapped value as a confidence value.

They removed the top 2% of rated movies to improve novelty and serendipity, metrics often ignored by other papers. This seems like a good idea and a good direction to make recommendations better. It is easy to recommend the most popular movie of all time, but most people would have heard about it by now, meaning the recommendation has very little value. It is unclear how it exactly helps with serendipity, though, maybe because people tend to watch popular movies.

It would have been better if the explicitly wrote the formula for the biases. They reference a paper, but they use b_ui in their formulas without ever saying how it is actually calculated. It shouldn't be necessary to check another paper to have a complete formula for the calculations the authors did.

A criticism I have is with the way of testing. Choosing 1 rated item and 1000 unrated items, with the goal that the algorithm ranks the only rated item higher than any other item, doesn't seem to align with the overall goal of making a ranking algorithm that well. First, the useful information is a very small sampling of all the algorithm does. It has to compare the 1001 and come up with a definite ranking for each, but we only care about the position of one item. Second, it doesn't directly optimize the ranking problem. Just as the algorithm could be correctly ranking the 1001 items, it could also only be finding _the_ rated item in there and ranking it on top, in a task more about finding a pattern between the rated items of the user and applying it to find a specific rated item in an unknown set. If these metrics were used to validate and tune hyperparameters, it could be that they were optimizing for a different task. 

In summary, this was probably a revolutionary paper that contradicted the assumption that minimizing RMSE meant better ranking and top-N performance. It proposed simple changes to existing algorithms, changes that may be useful in traditional rating problems. They also tried to improve novelty and serendipity by removing the top 2% of most popular movies. This is something most research didn't focus on, but probably should. They could have been more explicit with some parts of the formulas and I also have some reservations about the testing method. In particular, about how little feedback is given to the algorithm in comparison with all it had to do and if the task that this method demands is actually the task in hand. But it is, nonetheless, a huge contribution to recommender systems.